Automatic differentiation variational inference (ADVI) offers fast and
easy-to-use posterior approximation in multiple modern probabilistic programming
languages. However, its stochastic optimizer lacks clear convergence criteria
and requires tuning parameters. Moreover, ADVI inherits the poor posterior
uncertainty estimates of mean-field variational Bayes (MFVB).  We introduce
``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the
intractable MFVB objective with a fixed Monte Carlo approximation, a technique
known in the stochastic optimization literature as the ``sample average
approximation'' (SAA).  By optimizing an approximate but deterministic
objective, DADVI can use off-the-shelf second-order optimization, and, unlike
standard mean-field ADVI, is amenable to more accurate posterior covariances
via linear response (LR).  In contrast to existing worst-case theory, we show
that, on certain classes of common statistical problems, DADVI and the SAA can
perform well with relatively few samples even in very high dimensions, though we
also show that such favorable results cannot extend to variational
approximations that are too expressive relative to mean-field ADVI. We show on a
variety of real-world problems that DADVI reliably finds good solutions with
default settings (unlike ADVI) and, together with LR covariances, is typically
faster and more accurate than standard ADVI.
